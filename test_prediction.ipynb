{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.fasta_utils import *\n",
    "from utils.proteome_process import *\n",
    "from utils.prefetcher import *\n",
    "from utils.network_utils import *\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import hdbscan\n",
    "import faiss\n",
    "from sklearn.metrics.pairwise import cosine_distances,euclidean_distances\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"/media/microscopie-lcb/swapnesh/protein/embeddings/phages/1Sept2024_INPHARED_db_latest/\"\n",
    "\n",
    "db_accession_path = os.path.join(data_folder , 'db_accessions.npy')\n",
    "faiss_index_path = os.path.join(data_folder , \"ESM2_650m_1Sept24_650m.zarrfaiss_index.bin\")\n",
    "eps_values_path = os.path.join(data_folder , \"ESM2_650m_1Sept24_650m_eps_values_flat_clusters.npy\")\n",
    "HieVi_tree =  os.path.join(data_folder , \"ESM2_3b_1Sept24_3b.zarr.gexf\")\n",
    "HieVi_INPHARED_ordered_annotation = \"HieVi_INPHARED_ordered_annotation.csv\"\n",
    "\n",
    "\n",
    "htree = nx.read_gexf(HieVi_tree)\n",
    "db_accessions = np.load(db_accession_path)\n",
    "\n",
    "annotations = pd.read_csv(HieVi_INPHARED_ordered_annotation)\n",
    "annotations= annotations[annotations[\"Accession\"].isin(db_accessions)]\n",
    "annotations = annotations.set_index(\"Accession\").loc[db_accessions].reset_index()\n",
    "\n",
    "eps_values = np.load(eps_values_path)\n",
    "index = faiss.read_index(faiss_index_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/media/microscopie-lcb/swapnesh/protein/embeddings/phages/NewGenomes/HieVi_test_case.faa\"\n",
    "# @title Compute phage representations\n",
    "expt_name = \"MyPhages\"  # @param {type:\"string\"}\n",
    "expt_name = expt_name.replace(' ','_')\n",
    "output_folder = os.path.dirname(filename) + os.sep \n",
    "fasta_path = filename\n",
    "model_name = \"650m\" # This colab works for 650m only\n",
    "mode = \"mean\"\n",
    "query_zarr_path = os.path.join(output_folder,f\"{expt_name}_{model_name}.zarr\")\n",
    "#!python GenPhageRepresentationsESM2.py {expt_name} {output_folder} {fasta_path} {model_name} {mode}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_zarr_path = f\"{output_folder}{expt_name}_{model_name}.zarr\"\n",
    "\n",
    "query_zarr_store = zarr.open(query_zarr_path,'r')\n",
    "query_vectors = query_zarr_store['vectors_mean'][:]*1.0\n",
    "query_accessions = query_zarr_store['accessions'][:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required files\n",
    "'''\n",
    "Required files\n",
    "faiss_index for the right model\n",
    "the network gexf\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_folder = \"/media/microscopie-lcb/swapnesh/protein/embeddings/phages/1Sept2024_INPHARED_db_latest/\"\n",
    "faiss_index_path = data_folder +\"ESM2_650m_1Sept24_650m.zarrfaiss_index.bin\"\n",
    "query_zarr_path = data_folder +\"ESM2_650m_1Sept24_650m.zarr\"#f\"{expt_name}_{model_name}.zarr\"\n",
    "db_clusters_path = data_folder +\"ESM2_3b_1Sept24_3b_flat_clusters_3b.csv\"\n",
    "eps_values_path = data_folder +\"ESM2_650m_1Sept24_650m_eps_values_flat_clusters.npy\"\n",
    "db_zarr_path = data_folder +\"ESM2_650m_1Sept24_650m.zarr\"\n",
    "HieVi_INPHARED_ordered_annotation = \"HieVi_INPHARED_ordered_annotation.csv\"\n",
    "HieVi_tree = data_folder + \"ESM2_3b_1Sept24_3b.zarr.gexf\"\n",
    "\n",
    "htree = nx.read_gexf(HieVi_tree)\n",
    "\n",
    "zarr_store = zarr.open(db_zarr_path,'r')\n",
    "db_accessions = zarr_store['accessions'][:]\n",
    "\n",
    "annotations = pd.read_csv(HieVi_INPHARED_ordered_annotation)\n",
    "annotations= annotations[annotations[\"Accession\"].isin(db_accessions)]\n",
    "annotations = annotations.set_index(\"Accession\").loc[db_accessions].reset_index()\n",
    "\n",
    "eps_values = np.load(eps_values_path)\n",
    "\n",
    "\n",
    "index = faiss.read_index(faiss_index_path)\n",
    "\n",
    "hievi_cluster = pd.read_csv(db_clusters_path)\n",
    "hievi_cluster= hievi_cluster[hievi_cluster[\"Accession\"].isin(db_accessions)]\n",
    "hievi_cluster = hievi_cluster.set_index(\"Accession\").loc[db_accessions].reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/media/microscopie-lcb/swapnesh/protein/embeddings/phages/NewGenomes/HieVi_test_case.faa\"\n",
    "# @title Compute phage representations\n",
    "expt_name = \"MyPhages\"  # @param {type:\"string\"}\n",
    "output_folder = os.path.dirname(filename) + os.sep \n",
    "fasta_path = filename\n",
    "model_name = \"650m\" # This colab works for 650m only\n",
    "mode = \"mean\"\n",
    "#!python GenPhageRepresentationsESM2.py {expt_name} {output_folder} {fasta_path} {model_name} {mode}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_zarr_path = f\"{output_folder}{expt_name}_{model_name}.zarr\"\n",
    "\n",
    "query_zarr_store = zarr.open(query_zarr_path,'r')\n",
    "query_vectors = query_zarr_store['vectors_mean'][:]*1.0\n",
    "query_accessions = query_zarr_store['accessions'][:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71, 1280)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hievi_cluster_prefix = 'HC_p'\n",
    "k_neighbours = 64 # \n",
    "distance_threshold = 0.023 # \n",
    "distance_in_tree = 2\n",
    "\n",
    "distances, indices = index.search(query_vectors, 1)\n",
    "valid_idx = distances[:,0] < distance_threshold\n",
    "invalid_idx = np.logical_not(valid_idx)\n",
    "\n",
    "if len(np.where(invalid_idx)[0]):\n",
    "    print('Cannot classifiy: ' ,len(np.where(invalid_idx)[0]))\n",
    "    invalid_query_df = pd.DataFrame({\"Accession\": query_accessions[invalid_idx]})\n",
    "    invalid_query_df.to_csv(query_zarr_path[:-5] + \"_HieVi_Unclassifieds.csv\")\n",
    "\n",
    "D1 = np.squeeze(distances[valid_idx,0])\n",
    "I1 = np.squeeze(indices[valid_idx,0])\n",
    "\n",
    "all_indices = np.unique(np.array(I1))\n",
    "all_nearest_accessions = db_accessions[all_indices]\n",
    "\n",
    "new_accession = []\n",
    "for a in all_nearest_accessions:\n",
    "    new_accession += find_predecessor_and_leaves(htree, a,distance_in_tree)\n",
    "    \n",
    "\n",
    "all_nearest_accessions = np.unique(np.ravel(np.array(new_accession)))\n",
    "all_indices = np.array([np.where(db_accessions==a)[0] for a in all_nearest_accessions])\n",
    "\n",
    "subset_db_vectors = np.squeeze(np.array(zarr_store['vectors_mean'])[all_indices]*1.0)\n",
    "subset_db_vectors.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hievi_cluster_prefix = 'HC_p'\n",
    "k_neighbours = 64 # \n",
    "distance_threshold = 0.023 # \n",
    "\n",
    "distances, indices = index.search(query_vectors, 1)\n",
    "valid_idx = distances[:,0] < distance_threshold\n",
    "invalid_idx = np.logical_not(valid_idx)\n",
    "\n",
    "if len(np.where(invalid_idx)[0]):\n",
    "    print('Cannot classifiy: ' ,len(np.where(invalid_idx)[0]))\n",
    "    invalid_query_df = pd.DataFrame({\"Accession\": query_accessions[invalid_idx]})\n",
    "    invalid_query_df.to_csv(query_zarr_path[:-5] + \"_HieVi_Unclassifieds.csv\")\n",
    "\n",
    "D = np.squeeze(distances[valid_idx,0])\n",
    "I = np.squeeze(indices[valid_idx,0])\n",
    "\n",
    "D1, I1 = index.search(np.array(zarr_store['vectors_mean'])[I]*1.0, k_neighbours)\n",
    "\n",
    "all_indices = []\n",
    "for ds,idxs in zip(I1,D1):\n",
    "    for d,i in zip(ds,idxs):\n",
    "        if d < distance_threshold*10:\n",
    "            all_indices +=[i]\n",
    "\n",
    "\n",
    "all_indices = np.unique(np.array(all_indices+ list(I)))\n",
    "all_nearest_accessions = db_accessions[all_indices]\n",
    "\n",
    "new_accession = []\n",
    "for a in all_nearest_accessions:\n",
    "    new_accession += find_predecessor_and_leaves(htree, a,2)\n",
    "    \n",
    "\n",
    "all_nearest_accessions = np.unique(np.ravel(np.array(new_accession)))\n",
    "all_indices = np.array([np.where(db_accessions==a)[0] for a in all_nearest_accessions])\n",
    "\n",
    "subset_db_vectors = np.squeeze(np.array(zarr_store['vectors_mean'])[all_indices]*1.0)\n",
    "subset_db_vectors.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Combine query and nearest phages from database data\n",
    "\n",
    "# indices = np.unique(np.ravel(indices))\n",
    "# print(f\"Nearest neighbor search completed. Found {len(indices)} unique neighbors.\")\n",
    "# Get nearest accessions in tree\n",
    "nearest_accessions = annotations[annotations[\"Accession\"].isin(all_nearest_accessions)]\n",
    "nearest_accessions = nearest_accessions.set_index(\"Accession\").loc[all_nearest_accessions].reset_index()\n",
    "\n",
    "\n",
    "# Combine query and database data\n",
    "query_df = pd.DataFrame({\"Accession\": query_accessions[valid_idx]})\n",
    "annotation_df = pd.concat([nearest_accessions, query_df], axis=0)\n",
    "mprs = np.concatenate((subset_db_vectors, query_vectors[valid_idx]), axis=0)\n",
    "\n",
    "# Perform clustering\n",
    "# dist_scaled = euclidean_distances(mprs).astype(\"double\")\n",
    "# clusterer = hdbscan.HDBSCAN(\n",
    "#     min_cluster_size=2,\n",
    "#     n_jobs=32,\n",
    "#     min_samples=1,\n",
    "#     allow_single_cluster=False,\n",
    "#     cluster_selection_method=\"leaf\",\n",
    "#     metric=\"precomputed\",\n",
    "#     gen_min_span_tree=True\n",
    "# )\n",
    "# clusterer.fit(dist_scaled)\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=2,\n",
    "    min_samples=1,\n",
    "    allow_single_cluster=False,\n",
    "    cluster_selection_method=\"leaf\",\n",
    "    metric=\"euclidean\",\n",
    "    gen_min_span_tree=True\n",
    ")\n",
    "clusterer.fit(mprs)\n",
    "\n",
    "annotation_df[\"HieVi_granular_cluster\"] = clusterer.labels_\n",
    "for i,eps in enumerate(eps_values):\n",
    "    annotation_df[hievi_cluster_prefix+str(i)] = clusterer.dbscan_clustering(cut_distance=eps,min_cluster_size=2)\n",
    "#annotation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Generate Network\n",
    "\n",
    "\n",
    "min_lambda =13.00 # @param {type:\"slider\", min:-1, max:32, step:1}\n",
    "# Create and save network\n",
    "G = make_network(clusterer, annotation_df,min_lambda=min_lambda)\n",
    "nx.write_gexf(G, query_zarr_path[:-5] + \"_HieVi.gexf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esmfold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
